<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>decomon.models.backward_cloning &mdash; decomon main documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
        <script src="../../../_static/versions.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            decomon
          </a>
              <div class="version">
                main
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials.html#decomon-for-industrial-challenges-1">DECOMON for Industrial Challenges #1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/modules.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contribute.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/airbus/decomon">Github</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">decomon</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">decomon.models.backward_cloning</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for decomon.models.backward_cloning</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras.backend</span> <span class="k">as</span> <span class="nn">K</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Add</span><span class="p">,</span> <span class="n">Average</span><span class="p">,</span> <span class="n">Concatenate</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Conv2D</span> <span class="k">as</span> <span class="n">Conv</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">,</span> <span class="n">Permute</span><span class="p">,</span> <span class="n">Reshape</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.utils.generic_utils</span> <span class="kn">import</span> <span class="n">has_arg</span><span class="p">,</span> <span class="n">to_list</span>

<span class="kn">from</span> <span class="nn">decomon.backward_layers.backward_layers</span> <span class="kn">import</span> <span class="n">get_backward</span> <span class="k">as</span> <span class="n">get_backward_</span>
<span class="kn">from</span> <span class="nn">decomon.backward_layers.backward_merge</span> <span class="kn">import</span> <span class="n">BackwardMerge</span>
<span class="kn">from</span> <span class="nn">decomon.backward_layers.crown</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Convert_2_backward_mode</span><span class="p">,</span>
    <span class="n">Convert_2_mode</span><span class="p">,</span>
    <span class="n">Fuse</span><span class="p">,</span>
    <span class="n">MergeWithPrevious</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">decomon.backward_layers.utils</span> <span class="kn">import</span> <span class="n">merge_with_previous</span>
<span class="kn">from</span> <span class="nn">decomon.layers.core</span> <span class="kn">import</span> <span class="n">ForwardMode</span>
<span class="kn">from</span> <span class="nn">decomon.layers.utils</span> <span class="kn">import</span> <span class="n">softmax_to_linear</span> <span class="k">as</span> <span class="n">softmax_2_linear</span>
<span class="kn">from</span> <span class="nn">decomon.models.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">check_input_tensors_sequential</span><span class="p">,</span>
    <span class="n">get_depth_dict</span><span class="p">,</span>
    <span class="n">get_mode</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">decomon.utils</span> <span class="kn">import</span> <span class="n">get_lower</span><span class="p">,</span> <span class="n">get_upper</span>


<div class="viewcode-block" id="is_purely_linear"><a class="viewcode-back" href="../../../api/decomon.models.html#decomon.models.backward_cloning.is_purely_linear">[docs]</a><span class="k">def</span> <span class="nf">is_purely_linear</span><span class="p">(</span><span class="n">layer</span><span class="p">):</span>
    <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="p">(</span><span class="n">Reshape</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Permute</span><span class="p">)):</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="p">(</span><span class="n">Dense</span><span class="p">,</span> <span class="n">Conv</span><span class="p">)))</span> <span class="ow">and</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_config</span><span class="p">()[</span><span class="s2">&quot;activation&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="kc">False</span></div>


<div class="viewcode-block" id="get_input"><a class="viewcode-back" href="../../../api/decomon.models.html#decomon.models.backward_cloning.get_input">[docs]</a><span class="k">def</span> <span class="nf">get_input</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">,</span> <span class="n">forward_map</span><span class="p">,</span> <span class="n">output_map</span><span class="p">):</span>

    <span class="n">parents</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">parent_nodes</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">parents</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">input_tensors</span>

    <span class="n">layer</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">outbound_layer</span>

    <span class="k">if</span> <span class="n">is_purely_linear</span><span class="p">(</span><span class="n">layer</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[]</span>

    <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">parent</span> <span class="ow">in</span> <span class="n">parents</span><span class="p">:</span>

        <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">parent</span><span class="p">)</span> <span class="ow">in</span> <span class="n">output_map</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">+=</span> <span class="n">output_map</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">parent</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">def get_backward_from_forward(model,</span>
<span class="sd">                             input_tensors=None,</span>
<span class="sd">                             back_bounds=[],</span>
<span class="sd">                             output_map,</span>
<span class="sd">                             forward_map,</span>
<span class="sd">                             backward_map,</span>
<span class="sd">                             finetune=False,</span>
<span class="sd">                             joint=True,</span>
<span class="sd">                             layer_fn=None,</span>
<span class="sd">                             ):</span>


<span class="sd">   if not callable(layer_fn):</span>
<span class="sd">       raise ValueError(&quot;Expected `layer_fn` argument to be a callable.&quot;)</span>


<span class="sd">       # create input tensors</span>


<span class="sd">       # sort nodes from input to output</span>
<span class="sd">   dico_nodes = get_depth_dict(model)</span>
<span class="sd">   keys = [e for e in dico_nodes.keys()]</span>
<span class="sd">   keys.sort(reverse=True)</span>


<span class="sd">   output = input_tensors</span>
<span class="sd">   for depth in keys:</span>
<span class="sd">       nodes = dico_nodes[depth]</span>
<span class="sd">       for node in nodes:</span>


<span class="sd">           layer = node.outbound_layer</span>
<span class="sd">           # get input</span>
<span class="sd">           parents = node.parent_nodes</span>
<span class="sd">           if len(parents):</span>
<span class="sd">               output = []</span>
<span class="sd">               for parent in parents:</span>
<span class="sd">                   output += output_map[id(parent)]</span>


<span class="sd">           # retrieve layers</span>
<span class="sd">           if id(node) in forward_map.keys</span>
<span class="sd">               if id(node) in output_map.keys() and joint:</span>
<span class="sd">                   continue</span>








<span class="sd">def get_backward_model(</span>
<span class="sd">   model,</span>
<span class="sd">   input_tensors=None,</span>
<span class="sd">   back_bounds=[],</span>
<span class="sd">   input_dim=-1,</span>
<span class="sd">   convex_domain=None,</span>
<span class="sd">   IBP=True,</span>
<span class="sd">   forward=True,</span>
<span class="sd">   finetune=False,</span>
<span class="sd">   layer_fn = None,</span>
<span class="sd">   backward_map=None,</span>
<span class="sd">   forward_map=None,</span>
<span class="sd">   output_map=None,</span>
<span class="sd">   finetune=False,</span>
<span class="sd">   joint=True,</span>
<span class="sd">   rec=1,</span>
<span class="sd">   input_tensors=None</span>
<span class="sd">   **kwargs,</span>
<span class="sd">):</span>


<span class="sd">   if not callable(layer_fn):</span>
<span class="sd">       raise ValueError(&quot;Expected `layer_fn` argument to be a callable.&quot;)</span>


<span class="sd">       # create input tensors</span>


<span class="sd">       # sort nodes from input to output</span>
<span class="sd">   dico_nodes = get_depth_dict(model)</span>
<span class="sd">   keys = [e for e in dico_nodes.keys()]</span>
<span class="sd">   keys.sort(reverse=True)</span>




<span class="sd">   output = input_tensors</span>
<span class="sd">   for depth in keys:</span>
<span class="sd">       nodes = dico_nodes[depth]</span>
<span class="sd">       for node in nodes:</span>


<span class="sd">           layer = node.outbound_layer</span>
<span class="sd">           # get input</span>
<span class="sd">           parents = node.parent_nodes</span>
<span class="sd">           if len(parents):</span>
<span class="sd">               output=[]</span>
<span class="sd">               for parent in parents:</span>
<span class="sd">                   output+=output_map[id(parent)]</span>


<span class="sd">           # retrieve layers</span>
<span class="sd">           if id(node) in forward_map.keys</span>
<span class="sd">           if id(node) in output_map.keys() and joint:</span>
<span class="sd">               continue</span>
<span class="sd">&quot;&quot;&quot;</span>


<span class="sd">&quot;&quot;&quot;&quot;</span>
<span class="sd">def get_fuse(mode):</span>


<span class="sd">   def get_fuse_priv(inputs_):</span>


<span class="sd">       inputs = inputs_[:-4]</span>
<span class="sd">       backward_bounds = inputs_[-4:]</span>
<span class="sd">       w_out_u, b_out_u, w_out_l, b_out_l = backward_bounds</span>


<span class="sd">       if mode == ForwardMode.F_FORWARD:</span>
<span class="sd">           x_0, w_f_u, b_f_u, w_f_l, b_f_l = inputs</span>
<span class="sd">       elif mode == ForwardMode.F_HYBRID:</span>
<span class="sd">           x_0, u_c, w_f_u, b_f_u, l_c, w_f_l, b_f_l = inputs</span>
<span class="sd">       else:</span>
<span class="sd">           u_c, l_c = inputs</span>




<span class="sd">       if mode in [ForwardMode.F_HYBRID, ForwardMode.F_IBP]:</span>
<span class="sd">           u_c_ = get_upper_box(l_c, u_c, w_out_u, b_out_u)</span>
<span class="sd">           l_c_ = get_lower_box(l_c, u_c, w_out_l, b_out_l)</span>


<span class="sd">       if mode in [ForwardMode.F_HYBRID, ForwardMode.F_FORWARD]:</span>


<span class="sd">           w_u, b_u, w_l, b_l = merge_with_previous([w_f_u, b_f_u, w_f_l, b_f_l]+backward_bounds)</span>


<span class="sd">       if mode == ForwardMode.F_IBP:</span>
<span class="sd">           return [u_c_, l_c_]</span>
<span class="sd">       if mode == ForwardMode.F_FORWARD:</span>
<span class="sd">           return [x_0, w_u, b_u, w_l, b_l]</span>


<span class="sd">       if mode == ForwardMode.F_HYBRID:</span>
<span class="sd">           return [x_0, u_c_, w_u, b_u, l_c_, w_l, b_l]</span>


<span class="sd">   return LambdaLayer(get_fuse_priv())</span>




<span class="sd">def get_no_fuse(mode, convex_domain):</span>


<span class="sd">   def get_fuse_priv(inputs_):</span>


<span class="sd">       inputs = inputs_[:-4]</span>
<span class="sd">       backward_bounds = inputs_[-4:]</span>
<span class="sd">       w_out_u, b_out_u, w_out_l, b_out_l = backward_bounds</span>


<span class="sd">       if mode == ForwardMode.F_FORWARD:</span>
<span class="sd">           x_0, w_f_u, b_f_u, w_f_l, b_f_l = inputs</span>
<span class="sd">           u_c_ = get_upper(x_0, w_f_u, b_f_u, convex_domain)</span>
<span class="sd">           l_c_ = get_lower(x_0, w_f_l, b_f_l, convex_domain)</span>
<span class="sd">       elif mode == ForwardMode.F_HYBRID:</span>
<span class="sd">           _, u_c, _, _, l_c, _, _ = inputs</span>
<span class="sd">       else:</span>
<span class="sd">           u_c, l_c = inputs</span>


<span class="sd">       x_ = K.concatenate([K.expand_dims(l_c_, 1), K.expand_dims(u_c_, 1)], 1)</span>








<span class="sd">       if mode in [ForwardMode.F_HYBRID, ForwardMode.F_IBP]:</span>
<span class="sd">           u_c_ = get_upper_box(l_c, u_c, w_out_u, b_out_u)</span>
<span class="sd">           l_c_ = get_lower_box(l_c, u_c, w_out_l, b_out_l)</span>


<span class="sd">       if mode in [ForwardMode.F_HYBRID, ForwardMode.F_FORWARD]:</span>


<span class="sd">           w_u, b_u, w_l, b_l = merge_with_previous([w_f_u, b_f_u, w_f_l, b_f_l]+backward_bounds)</span>


<span class="sd">       if mode == ForwardMode.F_IBP:</span>
<span class="sd">           return [u_c_, l_c_]</span>
<span class="sd">       if mode == ForwardMode.F_FORWARD:</span>
<span class="sd">           return [x_0, w_u, b_u, w_l, b_l]</span>


<span class="sd">       if mode == ForwardMode.F_HYBRID:</span>
<span class="sd">           return [x_0, u_c_, w_u, b_u, l_c_, w_l, b_l]</span>


<span class="sd">   return LambdaLayer(get_fuse_priv())</span>
<span class="sd">&quot;&quot;&quot;</span>


<div class="viewcode-block" id="get_fuse"><a class="viewcode-back" href="../../../api/decomon.models.html#decomon.models.backward_cloning.get_fuse">[docs]</a><span class="k">def</span> <span class="nf">get_fuse</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">floatx</span><span class="p">()):</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="n">ForwardMode</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_fuse_priv</span><span class="p">(</span><span class="n">inputs_</span><span class="p">):</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs_</span><span class="p">[:</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span>
        <span class="n">backward_bounds</span> <span class="o">=</span> <span class="n">inputs_</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:]</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">ForwardMode</span><span class="o">.</span><span class="n">AFFINE</span><span class="p">:</span>
            <span class="n">x_0</span><span class="p">,</span> <span class="n">w_f_u</span><span class="p">,</span> <span class="n">b_f_u</span><span class="p">,</span> <span class="n">w_f_l</span><span class="p">,</span> <span class="n">b_f_l</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">ForwardMode</span><span class="o">.</span><span class="n">HYBRID</span><span class="p">:</span>
            <span class="n">x_0</span><span class="p">,</span> <span class="n">u_c</span><span class="p">,</span> <span class="n">w_f_u</span><span class="p">,</span> <span class="n">b_f_u</span><span class="p">,</span> <span class="n">l_c</span><span class="p">,</span> <span class="n">w_f_l</span><span class="p">,</span> <span class="n">b_f_l</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">backward_bounds</span>

        <span class="k">return</span> <span class="n">merge_with_previous</span><span class="p">([</span><span class="n">w_f_u</span><span class="p">,</span> <span class="n">b_f_u</span><span class="p">,</span> <span class="n">w_f_l</span><span class="p">,</span> <span class="n">b_f_l</span><span class="p">]</span> <span class="o">+</span> <span class="n">backward_bounds</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">Lambda</span><span class="p">(</span><span class="n">get_fuse_priv</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="convert_backward_2_mode"><a class="viewcode-back" href="../../../api/decomon.models.html#decomon.models.backward_cloning.convert_backward_2_mode">[docs]</a><span class="k">def</span> <span class="nf">convert_backward_2_mode</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">convex_domain</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">floatx</span><span class="p">()):</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="n">ForwardMode</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_2_mode_priv</span><span class="p">(</span><span class="n">inputs_</span><span class="p">):</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs_</span><span class="p">[:</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span>
        <span class="n">backward_bounds</span> <span class="o">=</span> <span class="n">inputs_</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:]</span>
        <span class="n">w_out_u</span><span class="p">,</span> <span class="n">b_out_u</span><span class="p">,</span> <span class="n">w_out_l</span><span class="p">,</span> <span class="n">b_out_l</span> <span class="o">=</span> <span class="n">backward_bounds</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="ow">in</span> <span class="p">[</span><span class="n">ForwardMode</span><span class="o">.</span><span class="n">AFFINE</span><span class="p">,</span> <span class="n">ForwardMode</span><span class="o">.</span><span class="n">HYBRID</span><span class="p">]:</span>
            <span class="n">x_0</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">u_c</span><span class="p">,</span> <span class="n">l_c</span> <span class="o">=</span> <span class="n">inputs</span>
            <span class="n">x_0</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">K</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">l_c</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">K</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">u_c</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">ForwardMode</span><span class="o">.</span><span class="n">AFFINE</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">x_0</span><span class="p">]</span> <span class="o">+</span> <span class="n">backward_bounds</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">ForwardMode</span><span class="o">.</span><span class="n">IBP</span><span class="p">:</span>
            <span class="n">u_c_</span> <span class="o">=</span> <span class="n">get_upper</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">w_out_u</span><span class="p">,</span> <span class="n">b_out_u</span><span class="p">,</span> <span class="n">convex_domain</span><span class="o">=</span><span class="p">{})</span>
            <span class="n">l_c_</span> <span class="o">=</span> <span class="n">get_lower</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">w_out_l</span><span class="p">,</span> <span class="n">b_out_l</span><span class="p">,</span> <span class="n">convex_domain</span><span class="o">=</span><span class="p">{})</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">u_c_</span><span class="p">,</span> <span class="n">l_c_</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">ForwardMode</span><span class="o">.</span><span class="n">HYBRID</span><span class="p">:</span>
            <span class="n">u_c_</span> <span class="o">=</span> <span class="n">get_upper</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">w_out_u</span><span class="p">,</span> <span class="n">b_out_u</span><span class="p">,</span> <span class="n">convex_domain</span><span class="o">=</span><span class="n">convex_domain</span><span class="p">)</span>
            <span class="n">l_c_</span> <span class="o">=</span> <span class="n">get_lower</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">w_out_l</span><span class="p">,</span> <span class="n">b_out_l</span><span class="p">,</span> <span class="n">convex_domain</span><span class="o">=</span><span class="n">convex_domain</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">x_0</span><span class="p">,</span> <span class="n">u_c_</span><span class="p">,</span> <span class="n">w_out_u</span><span class="p">,</span> <span class="n">b_out_u</span><span class="p">,</span> <span class="n">l_c_</span><span class="p">,</span> <span class="n">w_out_l</span><span class="p">,</span> <span class="n">b_out_l</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">Lambda</span><span class="p">(</span><span class="n">get_2_mode_priv</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>

    <span class="c1"># return convert_2_mode(mode_from=ForwardMode.F_FORWARD, mode_to=mode, convex_domain=convex_domain)</span>


<div class="viewcode-block" id="get_disconnected_input"><a class="viewcode-back" href="../../../api/decomon.models.html#decomon.models.backward_cloning.get_disconnected_input">[docs]</a><span class="k">def</span> <span class="nf">get_disconnected_input</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">convex_domain</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">floatx</span><span class="p">()):</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="n">ForwardMode</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">disco_priv</span><span class="p">(</span><span class="n">inputs_</span><span class="p">):</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">ForwardMode</span><span class="o">.</span><span class="n">IBP</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">inputs_</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">ForwardMode</span><span class="o">.</span><span class="n">AFFINE</span><span class="p">:</span>
            <span class="n">x_0</span><span class="p">,</span> <span class="n">w_f_u</span><span class="p">,</span> <span class="n">b_f_u</span><span class="p">,</span> <span class="n">w_f_l</span><span class="p">,</span> <span class="n">b_f_l</span> <span class="o">=</span> <span class="n">inputs_</span>
            <span class="n">u_c</span> <span class="o">=</span> <span class="n">get_upper</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">w_f_u</span><span class="p">,</span> <span class="n">b_f_u</span><span class="p">,</span> <span class="n">convex_domain</span><span class="o">=</span><span class="n">convex_domain</span><span class="p">)</span>
            <span class="n">l_c</span> <span class="o">=</span> <span class="n">get_lower</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">w_f_l</span><span class="p">,</span> <span class="n">b_f_l</span><span class="p">,</span> <span class="n">convex_domain</span><span class="o">=</span><span class="n">convex_domain</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">ForwardMode</span><span class="o">.</span><span class="n">HYBRID</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">u_c</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">l_c</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">inputs_</span>

        <span class="n">x_0</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">K</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">l_c</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">K</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">u_c</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">w_u_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">x_0</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">u_c</span> <span class="o">+</span> <span class="n">K</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">x_0</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="n">b_u_</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">x_0</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">u_c</span>
        <span class="c1"># w_u_ = tf.linalg.diag(K.cast(0., x_0.dtype)*u_c)</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">ForwardMode</span><span class="o">.</span><span class="n">AFFINE</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">x_0</span><span class="p">,</span> <span class="n">w_u_</span><span class="p">,</span> <span class="n">b_u_</span><span class="p">,</span> <span class="n">w_u_</span><span class="p">,</span> <span class="n">b_u_</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">ForwardMode</span><span class="o">.</span><span class="n">HYBRID</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">x_0</span><span class="p">,</span> <span class="n">u_c</span><span class="p">,</span> <span class="n">w_u_</span><span class="p">,</span> <span class="n">b_u_</span><span class="p">,</span> <span class="n">l_c</span><span class="p">,</span> <span class="n">w_u_</span><span class="p">,</span> <span class="n">b_u_</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">Lambda</span><span class="p">(</span><span class="n">disco_priv</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="retrieve_layer"><a class="viewcode-back" href="../../../api/decomon.models.html#decomon.models.backward_cloning.retrieve_layer">[docs]</a><span class="k">def</span> <span class="nf">retrieve_layer</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">previous</span><span class="p">,</span> <span class="n">layer_fn</span><span class="p">,</span> <span class="n">backward_map</span><span class="p">,</span> <span class="n">joint</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">retrieve_layer_priv</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">previous</span><span class="p">,</span> <span class="n">pattern</span><span class="p">,</span> <span class="n">layer_fn</span><span class="p">,</span> <span class="n">backward_map</span><span class="p">,</span> <span class="n">joint</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">node</span><span class="p">)</span> <span class="ow">in</span> <span class="n">backward_map</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="ow">and</span> <span class="n">pattern</span> <span class="ow">in</span> <span class="n">backward_map</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">node</span><span class="p">)]</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">backward_layer</span> <span class="o">=</span> <span class="n">backward_map</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">node</span><span class="p">)][</span><span class="n">pattern</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">backward_layer</span> <span class="o">=</span> <span class="n">layer_fn</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">outbound_layer</span><span class="p">)</span>
            <span class="n">backward_layer</span><span class="o">.</span><span class="n">previous</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">previous</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">joint</span><span class="p">:</span>
                <span class="c1"># no need to create another object at the next call</span>
                <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">node</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">backward_map</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                    <span class="n">backward_map</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">node</span><span class="p">)]</span> <span class="o">=</span> <span class="p">{</span><span class="n">pattern</span><span class="p">:</span> <span class="n">backward_layer</span><span class="p">}</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">backward_map</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">node</span><span class="p">)][</span><span class="n">pattern</span><span class="p">]</span> <span class="o">=</span> <span class="n">backward_layer</span>
        <span class="k">return</span> <span class="n">backward_layer</span><span class="p">,</span> <span class="n">backward_map</span>

    <span class="k">if</span> <span class="n">previous</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">retrieve_layer_priv</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">previous</span><span class="p">,</span> <span class="s2">&quot;previous&quot;</span><span class="p">,</span> <span class="n">layer_fn</span><span class="p">,</span> <span class="n">backward_map</span><span class="p">,</span> <span class="n">joint</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">retrieve_layer_priv</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">previous</span><span class="p">,</span> <span class="s2">&quot;no_previous&quot;</span><span class="p">,</span> <span class="n">layer_fn</span><span class="p">,</span> <span class="n">backward_map</span><span class="p">,</span> <span class="n">joint</span><span class="p">)</span></div>


<div class="viewcode-block" id="crown_"><a class="viewcode-back" href="../../../api/decomon.models.html#decomon.models.backward_cloning.crown_">[docs]</a><span class="k">def</span> <span class="nf">crown_</span><span class="p">(</span>
    <span class="n">node</span><span class="p">,</span>
    <span class="n">IBP</span><span class="p">,</span>
    <span class="n">forward</span><span class="p">,</span>
    <span class="n">convex_domain</span><span class="p">,</span>
    <span class="n">input_map</span><span class="p">,</span>
    <span class="n">layer_fn</span><span class="p">,</span>
    <span class="n">backward_bounds</span><span class="p">,</span>
    <span class="n">backward_map</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">joint</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">fuse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">output_map</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">merge_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">fuse_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>


<span class="sd">    :param node:</span>
<span class="sd">    :param IBP:</span>
<span class="sd">    :param forward:</span>
<span class="sd">    :param input_map:</span>
<span class="sd">    :param layer_fn:</span>
<span class="sd">    :param backward_bounds:</span>
<span class="sd">    :param backward_map:</span>
<span class="sd">    :param joint:</span>
<span class="sd">    :param fuse:</span>
<span class="sd">    :return: list of 4 tensors affine upper and lower bounds</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">backward_map</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">backward_map</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="n">output_map</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output_map</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="n">input_map</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">node</span><span class="p">)]</span>

    <span class="k">if</span> <span class="n">convex_domain</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">convex_domain</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="n">merge_layers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># merge_layers = Lambda(merge_with_previous)</span>
        <span class="c1"># merge_layers = MergeWithPrevious()</span>
        <span class="n">merge_layers</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">outbound_layer</span><span class="p">,</span> <span class="n">Model</span><span class="p">):</span>

        <span class="n">inputs_</span> <span class="o">=</span> <span class="n">get_disconnected_input</span><span class="p">(</span><span class="n">get_mode</span><span class="p">(</span><span class="n">IBP</span><span class="p">,</span> <span class="n">forward</span><span class="p">),</span> <span class="n">convex_domain</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;debug&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">backward_bounds_</span><span class="p">,</span> <span class="n">backward_map_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">crown_model</span><span class="p">(</span>
            <span class="n">node</span><span class="o">.</span><span class="n">outbound_layer</span><span class="p">,</span>
            <span class="n">input_tensors</span><span class="o">=</span><span class="n">inputs_</span><span class="p">,</span>
            <span class="n">backward_bounds</span><span class="o">=</span><span class="n">backward_bounds</span><span class="p">,</span>
            <span class="n">IBP</span><span class="o">=</span><span class="n">IBP</span><span class="p">,</span>
            <span class="n">forward</span><span class="o">=</span><span class="n">forward</span><span class="p">,</span>
            <span class="n">convex_domain</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">finetune</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">joint</span><span class="o">=</span><span class="n">joint</span><span class="p">,</span>
            <span class="n">fuse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">backward_layer</span><span class="p">,</span> <span class="n">backward_map_</span> <span class="o">=</span> <span class="n">retrieve_layer</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">backward_bounds</span><span class="p">),</span> <span class="n">layer_fn</span><span class="p">,</span> <span class="n">backward_map</span><span class="p">,</span> <span class="n">joint</span><span class="p">)</span>
        <span class="n">backward_map</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">backward_map_</span><span class="p">)</span>
        <span class="c1"># to retrieve</span>
        <span class="c1"># backward_bounds_ = backward_layer.call_no_previous(inputs)</span>
        <span class="c1"># update back_bounds</span>
        <span class="c1"># try:</span>
        <span class="c1"># backward_layer.previous=False</span>
        <span class="c1"># backward_tmp = [r for r in backward_bounds]</span>
        <span class="c1"># backward_bounds_ = backward_layer.call_no_previous(inputs)</span>

        <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">node</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_map</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">backward_layer</span><span class="o">.</span><span class="n">previous</span><span class="p">:</span>
                <span class="n">backward_layer</span><span class="o">.</span><span class="n">previous</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">backward_bounds_</span> <span class="o">=</span> <span class="n">backward_layer</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">output_map</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">node</span><span class="p">)]</span> <span class="o">=</span> <span class="n">backward_bounds_</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">backward_bounds_</span> <span class="o">=</span> <span class="n">output_map</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">node</span><span class="p">)]</span>

            <span class="c1"># backward_bounds_ = backward_layer.call_no_previous(inputs)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">backward_bounds_</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">backward_bounds_</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">backward_bounds_</span><span class="p">]</span>
            <span class="c1"># import pdb; pdb.set_trace()</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">backward_bounds</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">merge_layers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">merge_layers</span> <span class="o">=</span> <span class="n">MergeWithPrevious</span><span class="p">(</span><span class="n">backward_bounds_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">backward_bounds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">backward_tmp</span> <span class="o">=</span> <span class="n">merge_layers</span><span class="p">(</span><span class="n">backward_bounds_</span> <span class="o">+</span> <span class="n">backward_bounds</span><span class="p">)</span>
            <span class="n">backward_bounds_</span> <span class="o">=</span> <span class="n">backward_tmp</span>

            <span class="c1"># backward_bounds_ = \</span>
            <span class="c1">#    backward_layer(inputs+backward_bounds)</span>
            <span class="c1"># debugging</span>

            <span class="c1"># backward_layer.previous = False</span>
            <span class="c1"># tmp = backward_layer.call_no_previous(inputs)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">backward_bounds_</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">backward_bounds</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">backward_bounds_</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">backward_bounds</span> <span class="o">=</span> <span class="n">backward_bounds_</span>

    <span class="n">parents</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">parent_nodes</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">parents</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        if isinstance(node.layer, _Merge):</span>
<span class="sd">            crown_bounds=[]</span>
<span class="sd">            for b_bounds, parent in zip(backward_bounds, parents):</span>
<span class="sd">                tmp, backward_map_ = crown_(parent, IBP, forward, input_map, layer_fn, b_bounds, backward_map, joint, fuse)</span>
<span class="sd">                backward_map.update(backward_map_)</span>
<span class="sd">                crown_bounds.append(tmp)</span>


<span class="sd">            return crown_bounds[0], backward_map #TODO optimize with minimum !!!</span>
<span class="sd">        else:</span>
<span class="sd">            if len(parents)&gt;1:</span>
<span class="sd">                raise NotImplementedError()</span>
<span class="sd">            crown_bound, backward_map_ = crown_(parents[0], IBP, forward, input_map, layer_fn, backward_bounds, backward_map, joint, fuse)</span>
<span class="sd">            backward_map.update(backward_map_)</span>
<span class="sd">            return crown_bound, backward_map</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">parents</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">backward_layer</span><span class="p">,</span> <span class="n">BackwardMerge</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
                <span class="n">crown_bound_list</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">backward_bound</span><span class="p">,</span> <span class="n">parent</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">backward_bounds</span><span class="p">,</span> <span class="n">parents</span><span class="p">):</span>

                    <span class="n">crown_bound_i</span><span class="p">,</span> <span class="n">backward_map_i</span> <span class="o">=</span> <span class="n">crown_</span><span class="p">(</span>
                        <span class="n">parent</span><span class="p">,</span>
                        <span class="n">IBP</span><span class="p">,</span>
                        <span class="n">forward</span><span class="p">,</span>
                        <span class="n">convex_domain</span><span class="p">,</span>
                        <span class="n">input_map</span><span class="p">,</span>
                        <span class="n">layer_fn</span><span class="p">,</span>
                        <span class="n">backward_bound</span><span class="p">,</span>
                        <span class="n">backward_map</span><span class="p">,</span>
                        <span class="n">joint</span><span class="p">,</span>
                        <span class="n">fuse</span><span class="p">,</span>
                    <span class="p">)</span>

                    <span class="n">crown_bound_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">crown_bound_i</span><span class="p">)</span>

                <span class="c1"># import pdb; pdb.set_trace()</span>
                <span class="n">avg_layer</span> <span class="o">=</span> <span class="n">Average</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">node</span><span class="o">.</span><span class="n">outbound_layer</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

                <span class="c1"># crown_bound = [avg_layer([e[i] for e in crown_bound_list]) for i in range(4)]</span>
                <span class="n">crown_bound</span> <span class="o">=</span> <span class="n">crown_bound_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">crown_bound</span><span class="p">,</span> <span class="n">backward_map_</span><span class="p">,</span> <span class="n">output_map_</span><span class="p">,</span> <span class="n">fuse_layer_</span> <span class="o">=</span> <span class="n">crown_</span><span class="p">(</span>
                <span class="n">parents</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">IBP</span><span class="p">,</span>
                <span class="n">forward</span><span class="p">,</span>
                <span class="n">convex_domain</span><span class="p">,</span>
                <span class="n">input_map</span><span class="p">,</span>
                <span class="n">layer_fn</span><span class="p">,</span>
                <span class="n">backward_bounds</span><span class="p">,</span>
                <span class="n">backward_map</span><span class="p">,</span>
                <span class="n">joint</span><span class="p">,</span>
                <span class="n">fuse</span><span class="p">,</span>
                <span class="n">output_map</span><span class="o">=</span><span class="n">output_map</span><span class="p">,</span>
                <span class="n">merge_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># AKA merge_layers</span>
                <span class="n">fuse_layer</span><span class="o">=</span><span class="n">fuse_layer</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">backward_map</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">backward_map_</span><span class="p">)</span>
            <span class="n">output_map</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">output_map_</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">fuse_layer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">fuse_layer</span> <span class="o">=</span> <span class="n">fuse_layer_</span>
        <span class="k">return</span> <span class="n">crown_bound</span><span class="p">,</span> <span class="n">backward_map</span><span class="p">,</span> <span class="n">output_map</span><span class="p">,</span> <span class="n">fuse_layer</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># do something</span>
        <span class="k">if</span> <span class="n">fuse</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">fuse_layer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># fuse_layer = get_fuse(get_mode(IBP=IBP, forward=forward), dtype=inputs[0].dtype)</span>
                <span class="n">fuse_layer</span> <span class="o">=</span> <span class="n">Fuse</span><span class="p">(</span><span class="n">get_mode</span><span class="p">(</span><span class="n">IBP</span><span class="o">=</span><span class="n">IBP</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="n">forward</span><span class="p">))</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">fuse_layer</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">backward_bounds</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">result</span><span class="p">,</span> <span class="n">backward_map</span><span class="p">,</span> <span class="n">output_map</span><span class="p">,</span> <span class="n">fuse_layer</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">backward_bounds</span><span class="p">,</span> <span class="n">backward_map</span><span class="p">,</span> <span class="n">output_map</span><span class="p">,</span> <span class="n">fuse_layer</span></div>


<div class="viewcode-block" id="get_input_nodes"><a class="viewcode-back" href="../../../api/decomon.models.html#decomon.models.backward_cloning.get_input_nodes">[docs]</a><span class="k">def</span> <span class="nf">get_input_nodes</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">dico_nodes</span><span class="p">,</span>
    <span class="n">IBP</span><span class="p">,</span>
    <span class="n">forward</span><span class="p">,</span>
    <span class="n">input_tensors</span><span class="p">,</span>
    <span class="n">output_map</span><span class="p">,</span>
    <span class="n">layer_fn</span><span class="p">,</span>
    <span class="n">joint</span><span class="p">,</span>
    <span class="n">convex_domain</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">merge_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">set_mode_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>

    <span class="n">keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">dico_nodes</span><span class="o">.</span><span class="n">keys</span><span class="p">()]</span>
    <span class="n">keys</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">fuse_layer</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">input_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">backward_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">convex_domain</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">convex_domain</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">input_tensors</span>
    <span class="n">crown_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">:</span>
        <span class="n">nodes</span> <span class="o">=</span> <span class="n">dico_nodes</span><span class="p">[</span><span class="n">depth</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
            <span class="n">layer</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">outbound_layer</span>

            <span class="n">parents</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">parent_nodes</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">len</span><span class="p">(</span><span class="n">parents</span><span class="p">):</span>
                <span class="c1"># if &#39;debug&#39; in kwargs.keys():</span>
                <span class="c1">#    import pdb; pdb.set_trace()</span>
                <span class="n">input_map</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">node</span><span class="p">)]</span> <span class="o">=</span> <span class="n">input_tensors</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">is_purely_linear</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">outbound_layer</span><span class="p">):</span>
                    <span class="n">input_map</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">node</span><span class="p">)]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">for</span> <span class="n">parent</span> <span class="ow">in</span> <span class="n">parents</span><span class="p">:</span>
                        <span class="c1"># do something</span>
                        <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">parent</span><span class="p">)</span> <span class="ow">in</span> <span class="n">output_map</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                            <span class="n">output</span> <span class="o">+=</span> <span class="n">output_map</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">parent</span><span class="p">)]</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="c1"># if &#39;debug&#39; in kwargs.keys():</span>
                            <span class="c1">#    import pdb;</span>
                            <span class="c1">#    pdb.set_trace()</span>
                            <span class="k">if</span> <span class="n">merge_layers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                                <span class="c1"># merge_layers = Lambda(merge_with_previous)</span>
                                <span class="c1"># merge_layers = MergeWithPrevious()</span>
                                <span class="n">merge_layers</span> <span class="o">=</span> <span class="kc">None</span>

                            <span class="n">output_crown</span><span class="p">,</span> <span class="n">backward_map_</span><span class="p">,</span> <span class="n">crown_map_</span><span class="p">,</span> <span class="n">fuse_layer_</span> <span class="o">=</span> <span class="n">crown_</span><span class="p">(</span>
                                <span class="n">parent</span><span class="p">,</span>
                                <span class="n">IBP</span><span class="o">=</span><span class="n">IBP</span><span class="p">,</span>
                                <span class="n">forward</span><span class="o">=</span><span class="n">forward</span><span class="p">,</span>
                                <span class="n">input_map</span><span class="o">=</span><span class="n">input_map</span><span class="p">,</span>
                                <span class="n">layer_fn</span><span class="o">=</span><span class="n">layer_fn</span><span class="p">,</span>
                                <span class="n">backward_bounds</span><span class="o">=</span><span class="p">[],</span>
                                <span class="n">backward_map</span><span class="o">=</span><span class="n">backward_map</span><span class="p">,</span>
                                <span class="n">joint</span><span class="o">=</span><span class="n">joint</span><span class="p">,</span>
                                <span class="n">fuse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                <span class="n">convex_domain</span><span class="o">=</span><span class="n">convex_domain</span><span class="p">,</span>
                                <span class="n">output_map</span><span class="o">=</span><span class="n">crown_map</span><span class="p">,</span>
                                <span class="n">merge_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># AKA merge_layers</span>
                                <span class="n">fuse_layer</span><span class="o">=</span><span class="n">fuse_layer</span><span class="p">,</span>
                            <span class="p">)</span>
                            <span class="c1"># if &#39;debug&#39; in kwargs.keys():</span>
                            <span class="c1">#    pdb.set_trace()</span>
                            <span class="n">backward_map</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">backward_map_</span><span class="p">)</span>
                            <span class="n">crown_map</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">crown_map_</span><span class="p">)</span>
                            <span class="k">if</span> <span class="n">fuse_layer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                                <span class="n">fuse_layer</span> <span class="o">=</span> <span class="n">fuse_layer_</span>

                            <span class="c1"># convert output_crown in the right mode</span>
                            <span class="k">if</span> <span class="n">set_mode_layer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                                <span class="c1"># set_mode_layer = convert_backward_2_mode(</span>
                                <span class="c1">#    get_mode(IBP, forward), convex_domain, dtype=input_tensors[0].dtype</span>
                                <span class="c1"># )</span>
                                <span class="n">set_mode_layer</span> <span class="o">=</span> <span class="n">Convert_2_backward_mode</span><span class="p">(</span><span class="n">get_mode</span><span class="p">(</span><span class="n">IBP</span><span class="p">,</span> <span class="n">forward</span><span class="p">),</span> <span class="n">convex_domain</span><span class="p">)</span>
                            <span class="n">output_crown_</span> <span class="o">=</span> <span class="n">set_mode_layer</span><span class="p">(</span><span class="n">input_tensors</span> <span class="o">+</span> <span class="n">output_crown</span><span class="p">)</span>
                            <span class="n">output</span> <span class="o">+=</span> <span class="n">to_list</span><span class="p">(</span><span class="n">output_crown_</span><span class="p">)</span>
                            <span class="c1"># crown_map[id(parent)]=output_crown_</span>

                    <span class="n">input_map</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">node</span><span class="p">)]</span> <span class="o">=</span> <span class="n">output</span>
    <span class="k">return</span> <span class="n">input_map</span><span class="p">,</span> <span class="n">backward_map</span><span class="p">,</span> <span class="n">crown_map</span></div>


<div class="viewcode-block" id="crown_model"><a class="viewcode-back" href="../../../api/decomon.models.html#decomon.models.backward_cloning.crown_model">[docs]</a><span class="k">def</span> <span class="nf">crown_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">input_tensors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">back_bounds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">IBP</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">convex_domain</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">finetune</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">forward_map</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">softmax_to_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">joint</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">layer_fn</span><span class="o">=</span><span class="n">get_backward_</span><span class="p">,</span>
    <span class="n">fuse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">if</span> <span class="n">back_bounds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">back_bounds</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">forward_map</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">forward_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Model</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">()</span>
    <span class="c1"># import time</span>
    <span class="c1"># zero_time = time.process_time()</span>
    <span class="n">has_softmax</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">softmax_to_linear</span><span class="p">:</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">has_softmax</span> <span class="o">=</span> <span class="n">softmax_2_linear</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>  <span class="c1"># do better because you modify the model eventually</span>

    <span class="k">if</span> <span class="n">input_dim</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">input_dim_init</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_shape</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">input_dim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">:])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_dim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">input_dim_init</span> <span class="o">=</span> <span class="n">input_dim</span>
    <span class="k">if</span> <span class="n">input_tensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># check that the model has one input else</span>
        <span class="n">input_tensors</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">_input_layers</span><span class="p">)):</span>

            <span class="n">tmp</span> <span class="o">=</span> <span class="n">check_input_tensors_sequential</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">IBP</span><span class="p">,</span> <span class="n">forward</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">convex_domain</span><span class="p">)</span>
            <span class="n">input_tensors</span> <span class="o">+=</span> <span class="n">tmp</span>

    <span class="c1"># layer_fn</span>
    <span class="c1">##########</span>
    <span class="n">has_iter</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">layer_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_fn</span><span class="o">.</span><span class="vm">__code__</span><span class="o">.</span><span class="n">co_varnames</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="s2">&quot;layer&quot;</span> <span class="ow">in</span> <span class="n">layer_fn</span><span class="o">.</span><span class="vm">__code__</span><span class="o">.</span><span class="n">co_varnames</span><span class="p">:</span>
        <span class="n">has_iter</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_iter</span><span class="p">:</span>
        <span class="c1"># layer, slope = V_slope.name, previous = True, mode = ForwardMode.F_HYBRID, convex_domain = {}, finetune = False, rec = 1, ** kwargs</span>
        <span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">layer</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">get_backward_</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">get_mode</span><span class="p">(</span><span class="n">IBP</span><span class="p">,</span> <span class="n">forward</span><span class="p">),</span> <span class="n">finetune</span><span class="o">=</span><span class="n">finetune</span><span class="p">)</span>

        <span class="n">layer_fn</span> <span class="o">=</span> <span class="n">func</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">layer_fn</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected `layer_fn` argument to be a callable.&quot;</span><span class="p">)</span>
    <span class="c1">###############</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">back_bounds</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">to_list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">output</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="c1"># sort nodes from input to output</span>
    <span class="n">dico_nodes</span> <span class="o">=</span> <span class="n">get_depth_dict</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">dico_nodes</span><span class="o">.</span><span class="n">keys</span><span class="p">()]</span>
    <span class="n">keys</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># generate input_map</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">finetune</span><span class="p">:</span>
        <span class="n">joint</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="c1"># set_mode_layer = convert_backward_2_mode(get_mode(IBP, forward), convex_domain, dtype=input_tensors[0].dtype)</span>
    <span class="n">set_mode_layer</span> <span class="o">=</span> <span class="n">Convert_2_backward_mode</span><span class="p">(</span><span class="n">get_mode</span><span class="p">(</span><span class="n">IBP</span><span class="p">,</span> <span class="n">forward</span><span class="p">),</span> <span class="n">convex_domain</span><span class="p">)</span>

    <span class="n">input_map</span><span class="p">,</span> <span class="n">backward_map</span><span class="p">,</span> <span class="n">crown_map</span> <span class="o">=</span> <span class="n">get_input_nodes</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">dico_nodes</span><span class="p">,</span>
        <span class="n">IBP</span><span class="p">,</span>
        <span class="n">forward</span><span class="p">,</span>
        <span class="n">input_tensors</span><span class="p">,</span>
        <span class="n">forward_map</span><span class="p">,</span>
        <span class="n">layer_fn</span><span class="p">,</span>
        <span class="n">joint</span><span class="p">,</span>
        <span class="n">convex_domain</span><span class="o">=</span><span class="n">convex_domain</span><span class="p">,</span>
        <span class="n">set_mode_layer</span><span class="o">=</span><span class="n">set_mode_layer</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># time_1 = time.process_time()</span>
    <span class="c1"># print(&#39;step1&#39;, time_1-zero_time)</span>
    <span class="c1"># retrieve output nodes</span>
    <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">output_nodes</span> <span class="o">=</span> <span class="n">dico_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># the ordering may change</span>
    <span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span><span class="o">.</span><span class="n">_keras_history</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">to_list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">output</span><span class="p">)]</span>
    <span class="n">fuse_layer</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">output_name</span> <span class="ow">in</span> <span class="n">output_names</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">output_nodes</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">outbound_layer</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">output_name</span><span class="p">:</span>
                <span class="c1"># compute with crown</span>
                <span class="n">output_crown</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">fuse_layer</span> <span class="o">=</span> <span class="n">crown_</span><span class="p">(</span>
                    <span class="n">node</span><span class="p">,</span>
                    <span class="n">IBP</span><span class="o">=</span><span class="n">IBP</span><span class="p">,</span>
                    <span class="n">forward</span><span class="o">=</span><span class="n">forward</span><span class="p">,</span>
                    <span class="n">input_map</span><span class="o">=</span><span class="n">input_map</span><span class="p">,</span>
                    <span class="n">layer_fn</span><span class="o">=</span><span class="n">layer_fn</span><span class="p">,</span>
                    <span class="n">backward_bounds</span><span class="o">=</span><span class="n">back_bounds</span><span class="p">,</span>
                    <span class="n">backward_map</span><span class="o">=</span><span class="n">backward_map</span><span class="p">,</span>
                    <span class="n">joint</span><span class="o">=</span><span class="n">joint</span><span class="p">,</span>
                    <span class="n">fuse</span><span class="o">=</span><span class="n">fuse</span><span class="p">,</span>
                    <span class="n">convex_domain</span><span class="o">=</span><span class="n">convex_domain</span><span class="p">,</span>
                    <span class="n">output_map</span><span class="o">=</span><span class="n">crown_map</span><span class="p">,</span>
                    <span class="n">fuse_layer</span><span class="o">=</span><span class="n">fuse_layer</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="c1"># time_2 = time.process_time()</span>
                <span class="c1"># print(&#39;step2&#39;, time_2-time_1)</span>
                <span class="k">if</span> <span class="n">fuse</span><span class="p">:</span>
                    <span class="c1"># import pdb; pdb.set_trace()</span>
                    <span class="n">output</span> <span class="o">+=</span> <span class="n">to_list</span><span class="p">(</span><span class="n">set_mode_layer</span><span class="p">(</span><span class="n">input_tensors</span> <span class="o">+</span> <span class="n">output_crown</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">output</span> <span class="o">=</span> <span class="n">output_crown</span>

    <span class="k">return</span> <span class="n">input_tensors</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">backward_map</span><span class="p">,</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="get_backward_model"><a class="viewcode-back" href="../../../api/decomon.models.html#decomon.models.backward_cloning.get_backward_model">[docs]</a><span class="k">def</span> <span class="nf">get_backward_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">input_tensors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">back_bounds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">IBP</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">convex_domain</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">finetune</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">forward_map</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">softmax_to_linear</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">joint</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">layer_fn</span><span class="o">=</span><span class="n">get_backward_</span><span class="p">,</span>
    <span class="n">final_ibp</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">final_forward</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">if</span> <span class="n">back_bounds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">back_bounds</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">forward_map</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">forward_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">back_bounds</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">back_bounds</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">back_bounds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">bias</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">C</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
            <span class="n">back_bounds</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">bias</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">crown_model</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">input_tensors</span><span class="p">,</span>
        <span class="n">back_bounds</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">,</span>
        <span class="n">IBP</span><span class="p">,</span>
        <span class="n">forward</span><span class="p">,</span>
        <span class="n">convex_domain</span><span class="p">,</span>
        <span class="n">finetune</span><span class="p">,</span>
        <span class="n">forward_map</span><span class="p">,</span>
        <span class="n">softmax_to_linear</span><span class="p">,</span>
        <span class="n">joint</span><span class="p">,</span>
        <span class="n">layer_fn</span><span class="p">,</span>
        <span class="n">fuse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">input_tensors</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">backward_map</span><span class="p">,</span> <span class="n">toto</span> <span class="o">=</span> <span class="n">result</span>
    <span class="n">mode_from</span> <span class="o">=</span> <span class="n">get_mode</span><span class="p">(</span><span class="n">IBP</span><span class="p">,</span> <span class="n">forward</span><span class="p">)</span>
    <span class="n">mode_to</span> <span class="o">=</span> <span class="n">get_mode</span><span class="p">(</span><span class="n">final_ibp</span><span class="p">,</span> <span class="n">final_forward</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">Convert_2_mode</span><span class="p">(</span>
        <span class="n">mode_from</span><span class="o">=</span><span class="n">mode_from</span><span class="p">,</span>
        <span class="n">mode_to</span><span class="o">=</span><span class="n">mode_to</span><span class="p">,</span>
        <span class="n">convex_domain</span><span class="o">=</span><span class="n">convex_domain</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">output</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode_to</span> <span class="o">!=</span> <span class="n">mode_from</span> <span class="ow">and</span> <span class="n">mode_from</span> <span class="o">==</span> <span class="n">ForwardMode</span><span class="o">.</span><span class="n">IBP</span><span class="p">:</span>

        <span class="n">f_input</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">z</span><span class="p">:</span> <span class="n">Concatenate</span><span class="p">(</span><span class="mi">1</span><span class="p">)([</span><span class="n">z</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">z</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span> <span class="kc">None</span><span class="p">]]))</span>
        <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">f_input</span><span class="p">([</span><span class="n">input_tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
    <span class="k">return</span> <span class="n">input_tensors</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">backward_map</span><span class="p">,</span> <span class="n">toto</span></div>


<span class="c1"># Aliasing</span>
<span class="n">convert_backward_model</span> <span class="o">=</span> <span class="n">get_backward_model</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">import numpy as np</span>
<span class="sd">import tensorflow as tf</span>
<span class="sd">import tensorflow.keras.backend as K</span>
<span class="sd">from tensorflow.keras.layers import Lambda</span>
<span class="sd">from tensorflow.keras.models import Model</span>
<span class="sd">from tensorflow.python.keras.utils.generic_utils import to_list</span>

<span class="sd">from decomon.backward_layers.backward_layers import get_backward as get_backward_</span>
<span class="sd">from decomon.backward_layers.utils import merge_with_previous</span>
<span class="sd">from decomon.layers.core import F_FORWARD, F_HYBRID, F_IBP</span>
<span class="sd">from decomon.layers.utils import softmax_to_linear as softmax_2_linear</span>
<span class="sd">from decomon.models.utils import (</span>
<span class="sd">    check_input_tensors_sequential,</span>
<span class="sd">    convert_2_mode,</span>
<span class="sd">    get_depth_dict,</span>
<span class="sd">    get_mode,</span>
<span class="sd">)</span>
<span class="sd">from decomon.utils import get_lower, get_upper</span>


<span class="sd">def is_purely_linear(layer):</span>
<span class="sd">    return False</span>


<span class="sd">def get_input(node, input_tensors, forward_map, output_map):</span>

<span class="sd">    parents = node.parent_nodes</span>

<span class="sd">    if len(parents) == 0:</span>
<span class="sd">        return input_tensors</span>

<span class="sd">    layer = node.outbound_layer</span>

<span class="sd">    if is_purely_linear(layer):</span>
<span class="sd">        return []</span>

<span class="sd">    output = []</span>
<span class="sd">    for parent in parents:</span>

<span class="sd">        if id(parent) in output_map:</span>
<span class="sd">            output += output_map[id(parent)]</span>
<span class="sd">        else:</span>
<span class="sd">            raise NotImplementedError()</span>


<span class="sd">def get_fuse(mode, dtype=K.floatx()):</span>
<span class="sd">    def get_fuse_priv(inputs_):</span>

<span class="sd">        inputs = inputs_[:-4]</span>
<span class="sd">        backward_bounds = inputs_[-4:]</span>

<span class="sd">        if mode == ForwardMode.F_FORWARD:</span>
<span class="sd">            x_0, w_f_u, b_f_u, w_f_l, b_f_l = inputs</span>
<span class="sd">        elif mode == ForwardMode.F_HYBRID:</span>
<span class="sd">            x_0, u_c, w_f_u, b_f_u, l_c, w_f_l, b_f_l = inputs</span>
<span class="sd">        else:</span>
<span class="sd">            return backward_bounds</span>

<span class="sd">        return merge_with_previous([w_f_u, b_f_u, w_f_l, b_f_l] + backward_bounds)</span>

<span class="sd">    return Lambda(get_fuse_priv, dtype=dtype)</span>


<span class="sd">def convert_backward_2_mode(mode, convex_domain, dtype=K.floatx()):</span>
<span class="sd">    def get_2_mode_priv(inputs_):</span>

<span class="sd">        inputs = inputs_[:-4]</span>
<span class="sd">        backward_bounds = inputs_[-4:]</span>
<span class="sd">        w_out_u, b_out_u, w_out_l, b_out_l = backward_bounds</span>

<span class="sd">        if mode in [ForwardMode.F_FORWARD, ForwardMode.F_HYBRID]:</span>
<span class="sd">            x_0 = inputs[0]</span>
<span class="sd">        else:</span>
<span class="sd">            u_c, l_c = inputs</span>
<span class="sd">            x_0 = K.concatenate([K.expand_dims(l_c, 1), K.expand_dims(u_c, 1)], 1)</span>

<span class="sd">        if mode == ForwardMode.F_FORWARD:</span>
<span class="sd">            return [x_0] + backward_bounds</span>

<span class="sd">        if mode == ForwardMode.F_IBP:</span>
<span class="sd">            u_c_ = get_upper(x_0, w_out_u, b_out_u, convex_domain={})</span>
<span class="sd">            l_c_ = get_lower(x_0, w_out_l, b_out_l, convex_domain={})</span>
<span class="sd">            return [u_c_, l_c_]</span>

<span class="sd">        if mode == ForwardMode.F_HYBRID:</span>
<span class="sd">            u_c_ = get_upper(x_0, w_out_u, b_out_u, convex_domain=convex_domain)</span>
<span class="sd">            l_c_ = get_lower(x_0, w_out_l, b_out_l, convex_domain=convex_domain)</span>
<span class="sd">            return [x_0, u_c_, w_out_u, b_out_u, l_c_, w_out_l, b_out_l]</span>

<span class="sd">    return Lambda(get_2_mode_priv, dtype=dtype)</span>


<span class="sd">def get_disconnected_input(mode, convex_domain, dtype=K.floatx()):</span>
<span class="sd">    def disco_priv(inputs_):</span>

<span class="sd">        if mode == ForwardMode.F_IBP:</span>
<span class="sd">            return inputs_</span>
<span class="sd">        elif mode in {ForwardMode.F_FORWARD, ForwardMode.F_HYBRID}:</span>
<span class="sd">            if mode == ForwardMode.F_FORWARD:</span>
<span class="sd">                x_0, w_f_u, b_f_u, w_f_l, b_f_l = inputs_</span>
<span class="sd">                u_c = get_upper(x_0, w_f_u, b_f_u, convex_domain=convex_domain)</span>
<span class="sd">                l_c = get_lower(x_0, w_f_l, b_f_l, convex_domain=convex_domain)</span>
<span class="sd">            else:</span>
<span class="sd">                _, u_c, _, _, l_c, _, _ = inputs_</span>

<span class="sd">            x_0 = K.concatenate([K.expand_dims(l_c, 1), K.expand_dims(u_c, 1)], 1)</span>
<span class="sd">            w_u_ = tf.linalg.diag(K.cast(0.0, x_0.dtype) * u_c + K.cast(1.0, x_0.dtype))</span>
<span class="sd">            b_u_ = K.cast(0.0, x_0.dtype) * u_c</span>

<span class="sd">            if mode == ForwardMode.F_FORWARD:</span>
<span class="sd">                return [x_0, w_u_, b_u_, w_u_, b_u_]</span>
<span class="sd">            else:</span>
<span class="sd">                return [x_0, u_c, w_u_, b_u_, l_c, w_u_, b_u_]</span>
<span class="sd">        else:</span>
<span class="sd">            raise ValueError(f&quot;Unknown mode {mode}&quot;)</span>

<span class="sd">    return Lambda(disco_priv, dtype=dtype)</span>


<span class="sd">def retrieve_layer(node, previous, layer_fn, backward_map, joint=True):</span>
<span class="sd">    def retrieve_layer_priv(node, previous, pattern, layer_fn, backward_map, joint):</span>
<span class="sd">        if id(node) in backward_map.keys() and pattern in backward_map[id(node)].keys():</span>
<span class="sd">            backward_layer = backward_map[id(node)][pattern]</span>
<span class="sd">        else:</span>
<span class="sd">            backward_layer = layer_fn(node.outbound_layer)</span>
<span class="sd">            backward_layer.previous = bool(previous)</span>
<span class="sd">            if joint:</span>
<span class="sd">                # no need to create another object at the next call</span>
<span class="sd">                if id(node) not in backward_map.keys():</span>
<span class="sd">                    backward_map[id(node)] = {pattern: backward_layer}</span>
<span class="sd">                else:</span>
<span class="sd">                    backward_map[id(node)][pattern] = backward_layer</span>
<span class="sd">        return backward_layer, backward_map</span>

<span class="sd">    if previous:</span>
<span class="sd">        return retrieve_layer_priv(node, previous, &quot;previous&quot;, layer_fn, backward_map, joint)</span>
<span class="sd">    else:</span>
<span class="sd">        return retrieve_layer_priv(node, previous, &quot;no_previous&quot;, layer_fn, backward_map, joint)</span>


<span class="sd">def crown_(</span>
<span class="sd">    node,</span>
<span class="sd">    IBP,</span>
<span class="sd">    forward,</span>
<span class="sd">    convex_domain,</span>
<span class="sd">    input_map,</span>
<span class="sd">    layer_fn,</span>
<span class="sd">    backward_bounds,</span>
<span class="sd">    backward_map=None,</span>
<span class="sd">    joint=True,</span>
<span class="sd">    fuse=True,</span>
<span class="sd">    output_map=None,</span>
<span class="sd">    merge_layers=None,</span>
<span class="sd">    **kwargs,</span>
<span class="sd">):</span>
<span class="sd">    &quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        node</span>
<span class="sd">        IBP</span>
<span class="sd">        forward</span>
<span class="sd">        input_map</span>
<span class="sd">        layer_fn</span>
<span class="sd">        backward_bounds</span>
<span class="sd">        backward_map</span>
<span class="sd">        joint</span>
<span class="sd">        fuse</span>

<span class="sd">    Returns:</span>
<span class="sd">        list of 4 tensors affine upper and lower bounds</span>
<span class="sd">    &quot;&quot;</span>
<span class="sd">    if backward_map is None:</span>
<span class="sd">        backward_map = {}</span>

<span class="sd">    if output_map is None:</span>
<span class="sd">        output_map = {}</span>

<span class="sd">    inputs = input_map[id(node)]</span>

<span class="sd">    if convex_domain is None:</span>
<span class="sd">        convex_domain = {}</span>

<span class="sd">    if merge_layers is None:</span>
<span class="sd">        merge_layers = merge_with_previous</span>

<span class="sd">    if isinstance(node.outbound_layer, Model):</span>

<span class="sd">        inputs_ = get_disconnected_input(get_mode(IBP, forward), convex_domain, dtype=inputs[0].dtype)(inputs)</span>
<span class="sd">        kwargs.update({&quot;debug&quot;: True})</span>
<span class="sd">        _, backward_bounds_, backward_map_, _ = crown_model(</span>
<span class="sd">            node.outbound_layer,</span>
<span class="sd">            input_tensors=inputs_,</span>
<span class="sd">            backward_bounds=backward_bounds,</span>
<span class="sd">            IBP=IBP,</span>
<span class="sd">            forward=forward,</span>
<span class="sd">            convex_domain=None,</span>
<span class="sd">            finetune=False,</span>
<span class="sd">            joint=joint,</span>
<span class="sd">            fuse=False,</span>
<span class="sd">            **kwargs,</span>
<span class="sd">        )</span>

<span class="sd">    else:</span>
<span class="sd">        backward_layer, backward_map_ = retrieve_layer(node, len(backward_bounds), layer_fn, backward_map, joint)</span>
<span class="sd">        backward_map.update(backward_map_)</span>
<span class="sd">        # to retrieve</span>
<span class="sd">        backward_layer.previous= False</span>
<span class="sd">        #backward_bounds_ = backward_layer.call_no_previous(inputs)</span>
<span class="sd">        backward_bounds_ = backward_layer(inputs)</span>
<span class="sd">        # update back_bounds</span>
<span class="sd">        if id(node) not in output_map.keys():</span>
<span class="sd">            #backward_bounds_ = backward_layer.call_no_previous(inputs)</span>
<span class="sd">            backward_bounds_ = backward_layer(inputs)</span>
<span class="sd">            output_map[id(node)] = backward_bounds_</span>
<span class="sd">        else:</span>
<span class="sd">            backward_bounds_ = output_map[id(node)]</span>

<span class="sd">        if not isinstance(backward_bounds_, list):</span>
<span class="sd">            backward_bounds_ = [e for e in backward_bounds_]</span>
<span class="sd">        if len(backward_bounds):</span>
<span class="sd">            backward_tmp = merge_layers(backward_bounds_ + backward_bounds)</span>
<span class="sd">            backward_bounds_ = backward_tmp</span>

<span class="sd">    if not isinstance(backward_bounds_, list):</span>
<span class="sd">        backward_bounds = [e for e in backward_bounds_]</span>
<span class="sd">    else:</span>
<span class="sd">        backward_bounds = backward_bounds_</span>

<span class="sd">    parents = node.parent_nodes</span>

<span class="sd">    if len(parents):</span>
<span class="sd">        if len(parents) &gt; 1:</span>
<span class="sd">            raise NotImplementedError()</span>
<span class="sd">        else:</span>
<span class="sd">            crown_bound, backward_map_, output_map_ = crown_(</span>
<span class="sd">                parents[0],</span>
<span class="sd">                IBP,</span>
<span class="sd">                forward,</span>
<span class="sd">                convex_domain,</span>
<span class="sd">                input_map,</span>
<span class="sd">                layer_fn,</span>
<span class="sd">                backward_bounds,</span>
<span class="sd">                backward_map,</span>
<span class="sd">                joint,</span>
<span class="sd">                fuse,</span>
<span class="sd">                output_map=output_map,</span>
<span class="sd">                merge_layers=merge_layers,</span>
<span class="sd">            )</span>
<span class="sd">            backward_map.update(backward_map_)</span>
<span class="sd">            output_map.update(output_map_)</span>
<span class="sd">        return crown_bound, backward_map, output_map</span>
<span class="sd">    else:</span>
<span class="sd">        if fuse:</span>
<span class="sd">            fuse_layer = get_fuse(get_mode(IBP=IBP, forward=forward), dtype=inputs[0].dtype)</span>
<span class="sd">            result = fuse_layer(inputs + backward_bounds)</span>

<span class="sd">            return result, backward_map, output_map</span>

<span class="sd">        else:</span>
<span class="sd">            return backward_bounds, backward_map, output_map</span>


<span class="sd">def get_input_nodes(</span>
<span class="sd">    model,</span>
<span class="sd">    dico_nodes,</span>
<span class="sd">    IBP,</span>
<span class="sd">    forward,</span>
<span class="sd">    input_tensors,</span>
<span class="sd">    output_map,</span>
<span class="sd">    layer_fn,</span>
<span class="sd">    joint,</span>
<span class="sd">    convex_domain=None,</span>
<span class="sd">    merge_layers=None,</span>
<span class="sd">    **kwargs,</span>
<span class="sd">):</span>

<span class="sd">    keys = [e for e in dico_nodes.keys()]</span>
<span class="sd">    keys.sort(reverse=True)</span>
<span class="sd">    input_map = {}</span>
<span class="sd">    backward_map = {}</span>
<span class="sd">    if convex_domain is None:</span>
<span class="sd">        convex_domain = {}</span>
<span class="sd">    crown_map = {}</span>
<span class="sd">    set_mode_layer = None</span>
<span class="sd">    for depth in keys:</span>
<span class="sd">        nodes = dico_nodes[depth]</span>
<span class="sd">        for node in nodes:</span>
<span class="sd">            parents = node.parent_nodes</span>
<span class="sd">            if not len(parents):</span>
<span class="sd">                input_map[id(node)] = input_tensors</span>
<span class="sd">            else:</span>
<span class="sd">                if is_purely_linear(node.outbound_layer):</span>
<span class="sd">                    input_map[id(node)] = []</span>
<span class="sd">                else:</span>
<span class="sd">                    output = []</span>
<span class="sd">                    for parent in parents:</span>
<span class="sd">                        if id(parent) in output_map.keys():</span>
<span class="sd">                            output += output_map[id(parent)]</span>
<span class="sd">                        else:</span>
<span class="sd">                            if merge_layers is None:</span>
<span class="sd">                                merge_layers = merge_with_previous</span>
<span class="sd">                            output_crown, backward_map_, crown_map_ = crown_(</span>
<span class="sd">                                parent,</span>
<span class="sd">                                IBP=IBP,</span>
<span class="sd">                                forward=forward,</span>
<span class="sd">                                input_map=input_map,</span>
<span class="sd">                                layer_fn=layer_fn,</span>
<span class="sd">                                backward_bounds=[],</span>
<span class="sd">                                backward_map=backward_map,</span>
<span class="sd">                                joint=joint,</span>
<span class="sd">                                fuse=True,</span>
<span class="sd">                                convex_domain=convex_domain,</span>
<span class="sd">                                output_map=crown_map,</span>
<span class="sd">                                merge_layers=merge_layers,</span>
<span class="sd">                            )</span>
<span class="sd">                            backward_map.update(backward_map_)</span>
<span class="sd">                            crown_map.update(crown_map_)</span>

<span class="sd">                            # convert output_crown in the right mode</span>
<span class="sd">                            try:</span>
<span class="sd">                                if set_mode_layer is None:</span>
<span class="sd">                                    set_mode_layer = convert_backward_2_mode(</span>
<span class="sd">                                        get_mode(IBP, forward), convex_domain, dtype=input_tensors[0].dtype</span>
<span class="sd">                                    )</span>
<span class="sd">                                output_crown_ = set_mode_layer(input_tensors + output_crown)</span>
<span class="sd">                                output += to_list(output_crown_)</span>

<span class="sd">                            except TypeError:</span>
<span class="sd">                                pass</span>
<span class="sd">                    input_map[id(node)] = output</span>
<span class="sd">    return input_map, backward_map, crown_map</span>


<span class="sd">def crown_model(</span>
<span class="sd">    model,</span>
<span class="sd">    input_tensors=None,</span>
<span class="sd">    back_bounds=None,</span>
<span class="sd">    input_dim=-1,</span>
<span class="sd">    IBP=True,</span>
<span class="sd">    forward=True,</span>
<span class="sd">    convex_domain=None,</span>
<span class="sd">    finetune=False,</span>
<span class="sd">    forward_map=None,</span>
<span class="sd">    softmax_to_linear=True,</span>
<span class="sd">    joint=True,</span>
<span class="sd">    layer_fn=None,</span>
<span class="sd">    fuse=True,</span>
<span class="sd">    **kwargs,</span>
<span class="sd">):</span>
<span class="sd">    if back_bounds is None:</span>
<span class="sd">        back_bounds = []</span>
<span class="sd">    if forward_map is None:</span>
<span class="sd">        forward_map = {}</span>
<span class="sd">    if not isinstance(model, Model):</span>
<span class="sd">        raise ValueError()</span>
<span class="sd">    if softmax_to_linear:</span>
<span class="sd">        model, has_softmax = softmax_2_linear(model)</span>

<span class="sd">    if input_dim == -1:</span>
<span class="sd">        if isinstance(model.input_shape, list):</span>
<span class="sd">            input_dim = np.prod(model.input_shape[0][1:])</span>
<span class="sd">        else:</span>
<span class="sd">            input_dim = np.prod(model.input_shape[1:])</span>
<span class="sd">    if input_tensors is None:</span>
<span class="sd">        # check that the model has one input else</span>
<span class="sd">        input_tensors = []</span>
<span class="sd">        for i in range(len(model._input_layers)):</span>

<span class="sd">            tmp = check_input_tensors_sequential(model, None, input_dim, input_dim, IBP, forward, False, convex_domain)</span>
<span class="sd">            input_tensors += tmp</span>

<span class="sd">    # layer_fn</span>
<span class="sd">    ##########</span>
<span class="sd">    has_iter = False</span>

<span class="sd">    if layer_fn is not None and len(layer_fn.__code__.co_varnames) == 1 and &quot;layer&quot; in layer_fn.__code__.co_varnames:</span>
<span class="sd">        has_iter = True</span>

<span class="sd">    elif not has_iter:</span>
<span class="sd">        def func(layer):</span>
<span class="sd">            return get_backward_(layer, mode=get_mode(IBP, forward))</span>

<span class="sd">        layer_fn = func</span>

<span class="sd">    if not callable(layer_fn):</span>
<span class="sd">        raise ValueError(&quot;Expected `layer_fn` argument to be a callable.&quot;)</span>
<span class="sd">    ###############</span>

<span class="sd">    if len(back_bounds) and len(to_list(model.output)) &gt; 1:</span>
<span class="sd">        raise NotImplementedError()</span>

<span class="sd">    # sort nodes from input to output</span>
<span class="sd">    dico_nodes = get_depth_dict(model)</span>
<span class="sd">    keys = [e for e in dico_nodes.keys()]</span>
<span class="sd">    keys.sort(reverse=True)</span>

<span class="sd">    # generate input_map</span>
<span class="sd">    if not finetune:</span>
<span class="sd">        joint = True</span>

<span class="sd">    input_map, backward_map, crown_map = get_input_nodes(</span>
<span class="sd">        model,</span>
<span class="sd">        dico_nodes,</span>
<span class="sd">        IBP,</span>
<span class="sd">        forward,</span>
<span class="sd">        input_tensors,</span>
<span class="sd">        forward_map,</span>
<span class="sd">        layer_fn,</span>
<span class="sd">        joint,</span>
<span class="sd">        convex_domain=convex_domain,</span>
<span class="sd">        **kwargs,</span>
<span class="sd">    )</span>
<span class="sd">    # retrieve output nodes</span>
<span class="sd">    output = []</span>
<span class="sd">    output_nodes = dico_nodes[0]</span>
<span class="sd">    # the ordering may change</span>
<span class="sd">    output_names = [tensor._keras_history.layer.name for tensor in to_list(model.output)]</span>
<span class="sd">    for output_name in output_names:</span>
<span class="sd">        for node in output_nodes:</span>
<span class="sd">            if node.outbound_layer.name == output_name:</span>
<span class="sd">                # compute with crown</span>
<span class="sd">                output_crown, _, _ = crown_(</span>
<span class="sd">                    node,</span>
<span class="sd">                    IBP=IBP,</span>
<span class="sd">                    forward=forward,</span>
<span class="sd">                    input_map=input_map,</span>
<span class="sd">                    layer_fn=layer_fn,</span>
<span class="sd">                    backward_bounds=back_bounds,</span>
<span class="sd">                    backward_map=backward_map,</span>
<span class="sd">                    joint=joint,</span>
<span class="sd">                    fuse=fuse,</span>
<span class="sd">                    convex_domain=convex_domain,</span>
<span class="sd">                    output_map=crown_map,</span>
<span class="sd">                )</span>
<span class="sd">                if fuse:</span>
<span class="sd">                    output += to_list(</span>
<span class="sd">                        convert_backward_2_mode(get_mode(IBP, forward), convex_domain, dtype=input_tensors[0].dtype)(</span>
<span class="sd">                            input_tensors + output_crown</span>
<span class="sd">                        )</span>
<span class="sd">                    )</span>
<span class="sd">                else:</span>
<span class="sd">                    output = output_crown</span>

<span class="sd">    return input_tensors, output, backward_map, None</span>


<span class="sd">def get_backward_model(</span>
<span class="sd">    model,</span>
<span class="sd">    input_tensors=None,</span>
<span class="sd">    back_bounds=None,</span>
<span class="sd">    input_dim=-1,</span>
<span class="sd">    IBP=True,</span>
<span class="sd">    forward=True,</span>
<span class="sd">    convex_domain=None,</span>
<span class="sd">    finetune=False,</span>
<span class="sd">    forward_map=None,</span>
<span class="sd">    softmax_to_linear=True,</span>
<span class="sd">    joint=True,</span>
<span class="sd">    layer_fn=get_backward_,</span>
<span class="sd">    final_ibp=True,</span>
<span class="sd">    final_forward=False,</span>
<span class="sd">    **kwargs,</span>
<span class="sd">):</span>
<span class="sd">    if back_bounds is None:</span>
<span class="sd">        back_bounds = []</span>
<span class="sd">    if forward_map is None:</span>
<span class="sd">        forward_map = {}</span>
<span class="sd">    if len(back_bounds):</span>
<span class="sd">        if len(back_bounds) == 1:</span>
<span class="sd">            # TO DO: this step can be optimized by using the last layer</span>
<span class="sd">            C = back_bounds[0]</span>
<span class="sd">            bias = K.cast(0.0, model.layers[0].dtype) * C[:, 0]</span>
<span class="sd">            back_bounds = [C, bias] * 2</span>
<span class="sd">    result = crown_model(</span>
<span class="sd">        model,</span>
<span class="sd">        input_tensors,</span>
<span class="sd">        back_bounds,</span>
<span class="sd">        input_dim,</span>
<span class="sd">        IBP,</span>
<span class="sd">        forward,</span>
<span class="sd">        convex_domain,</span>
<span class="sd">        finetune,</span>
<span class="sd">        forward_map,</span>
<span class="sd">        softmax_to_linear,</span>
<span class="sd">        joint,</span>
<span class="sd">        layer_fn,</span>
<span class="sd">        fuse=True,</span>
<span class="sd">        **kwargs,</span>
<span class="sd">    )</span>

<span class="sd">    input_tensors, output, backward_map, toto = result</span>
<span class="sd">    output = convert_2_mode(</span>
<span class="sd">        mode_from=get_mode(IBP, forward),</span>
<span class="sd">        mode_to=get_mode(final_ibp, final_forward),</span>
<span class="sd">        convex_domain=convex_domain,</span>
<span class="sd">        dtype=model.layers[0].dtype,</span>
<span class="sd">    )(output)</span>
<span class="sd">    return input_tensors, output, backward_map, toto</span>


<span class="sd"># Aliasing</span>
<span class="sd">convert_backward_model = get_backward_model</span>
<span class="sd">&quot;&quot;&quot;</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Airbus.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Versions</span>
      v: main
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      <dl id="docs-versions">
      </dl>
    </div>
  </div>
<script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>